{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 로 LSTM 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow 를 사용하여 LSTM 을 만드는 방법 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 사용되는 input(data) 와 output(label) 종류\n",
    "    - input  : 길이 20의 2진수, ex) 00010011111111111111\n",
    "    - output : 길이 20의 2진수, ex) 00010100000000000000\n",
    "    - output 은 input 에 1을 더한 값\n",
    "        - 00010011111111111111 + 1 = 00010100000000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 길이 20의 2진수 문자열 2^20 개를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* data_size : data 의 크기, 2^20\n",
    "* train_set_size : data_set 에서 train 으로 사용될 data 의 양, 2^18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_size = 2**20\n",
    "train_set_size = 2**18\n",
    "\n",
    "data_set = ['{0:020b}'.format(i) for i in range(data_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 생성된 데이터 표본\n",
      "00000000000000000000\n",
      "00000000000000000001\n",
      "00000000000000000010\n",
      "00000000000000000011\n",
      "00000000000000000100\n",
      "00000000000000000101\n",
      "00000000000000000110\n",
      "00000000000000000111\n",
      "00000000000000001000\n",
      "00000000000000001001\n"
     ]
    }
   ],
   "source": [
    "print('* 생성된 데이터 표본')\n",
    "for data in data_set[:10]:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 길이 20의 배열로 분할하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2진수 문자열을 LSTM 에 적용하기 위하여 길이 20의 배열로 변환한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 문자열을 크기20의 배열 [20] 으로 분할한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ti  = []\n",
    "for i in data_set:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "            temp_list.append(j)\n",
    "    ti.append(temp_list)\n",
    "temp_data_set = ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 분할된 데이터 표본\n",
      "00000000000000000000 \n",
      "\t ->  ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "00000000000000000001 \n",
      "\t ->  ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1']\n",
      "00000000000000000010 \n",
      "\t ->  ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "print('* 분할된 데이터 표본')\n",
    "for origin, data in zip(data_set[:3], temp_data_set[:3]):\n",
    "    print(origin, '\\n\\t -> ', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. data 를 input 과 output 으로 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* output = input + 1 이므로, input 과 output 의 크기는 2^20 - 1 이다.\n",
    "    * input 범위 : 0 ~ 2^20-1\n",
    "    * output 범위 : 1 ~ 2^20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_data = []\n",
    "temp_target = []\n",
    "for idx in range(data_size - 1):\n",
    "    data = [int(val) for val in temp_data_set[idx][::-1]]\n",
    "    target = [int(val) for val in temp_data_set[idx+1][::-1]]\n",
    "    \n",
    "    temp_data.append(data)\n",
    "    temp_target.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_data = np.array(temp_data)\n",
    "temp_target = np.array(temp_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시계열 훈련을 위해 배열을 역순으로 반전 시킨후 저장\n",
    "* 이 과정이 없다면, 미래를 보고 과거를 맞추는 셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전 :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] , 후 :  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "전 :  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] , 후 :  [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "전 :  [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] , 후 :  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for data, label in zip(temp_data[:3], temp_target[:3]):\n",
    "    print('전 : ', data,', 후 : ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. shuffle 및 train, test 로 분리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* shuffle 이 더 효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "\n",
    "if shuffle == True:\n",
    "    indices = np.random.permutation(range(data_size-1))\n",
    "\n",
    "    temp_data = temp_data[indices]\n",
    "    temp_target = temp_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_input = temp_data[train_set_size:]\n",
    "test_output = temp_target[train_set_size:]\n",
    "train_input = temp_data[:train_set_size]\n",
    "train_output = temp_target[:train_set_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. 변수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* batch_size : input 으로 들어가는 배열의 크기, 2^9\n",
    "* step_size : 시계열의 길이, 20\n",
    "* class_size : output 의 label 종류, 0 or 1 - 2개\n",
    "* num_hidden : 한 cell 의 state 의 크기, 32\n",
    "* dropout_probs : dropout 확률, 1, 데이터의 특성상 한자리의 값도 빠지면 예측이 불가능하여 1로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2**9\n",
    "step_size = 20\n",
    "class_size = 2\n",
    "num_hidden = 32\n",
    "\n",
    "dropout_probs = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* placeholder : memory <-> graphic card, 사이의 데이터를 전송해주는 bridge 개념\n",
    "* x, y 를 통하여 그래픽카드 메모리에 input 과 output 을 전달할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [None, step_size])\n",
    "y = tf.placeholder(tf.int32, [None, step_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* embedding : word2vector 의 역할을 하는 함수\n",
    "    * [512, 20] -> [512, 20, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_size = 10\n",
    "\n",
    "embeddings = tf.get_variable('embedding_matrix', [class_size, vector_size])\n",
    "rnn_inputs = tf.nn.embedding_lookup(embeddings, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* embedding 처리 전의 input 모습\n",
      "(?, 20)\n",
      "* embedding 처리 후의 input 모습\n",
      "(?, 20, 10)\n"
     ]
    }
   ],
   "source": [
    "print('* embedding 처리 전의 input 모습')\n",
    "print(x.shape)\n",
    "print('* embedding 처리 후의 input 모습')\n",
    "print(rnn_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. LSTM Cell 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 32 크기의 state 를 가지는 LSTM Cell 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"single rnn cell.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* input 에 dropout 을 걸어준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=dropout_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3. MultiLayer RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cell 을 중첩하여 구성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"multi rnn cell.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs, _ = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. output 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 현재까지 기본이 되는 LSTM 을 구성하였다.\n",
    "* 앞으로는 Cell 을 구성하여 출력된 output 과 loss 를 어떻게 가공하느냐에 따라서, N to 1 혹은 N to N 으로 만들수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"rnn types.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    ▲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞으로의 코드는 \"many to one\" 의 RNN Cell 구성 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. state 를 output 으로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 32 개의 output state 를 \"0 or 1\" 의 20개 배열로 만들기 위하여, [32, 20 * 2] 의 weight 와 [20 * 2] 의 bias 를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([num_hidden, step_size * class_size]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[step_size * class_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 현재의 output 의 크기\n",
    "* [512, 20, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* outputs 에서 마지막 state 만 output 으로 사용하기 위해 크기를 변경한다\n",
    "    * [512, 20, 32] -> [20, 512, 32]\n",
    "* outputs 의 마지막 state 만 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "last_state = outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [512, 32] 의 last_state 만 연산한다\n",
    "* 연산된 logits 를 [10240, 2] 의 크기로 변경한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(last_state, weight) + bias\n",
    "logits = tf.reshape(logits, [-1, class_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 변경된 output 의 모습\n",
    "* [512, 32] -> [512, 40] -> [10240, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. loss 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [512, 20] 의 output 을 logits 의 크기에 맞춰 변경 -> [10240, 1]\n",
    "* logits 와 label 간의 cross_entropy 를 계산해주는 함수를 이용하여 total_loss 를 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_as_list = tf.reshape(y, [-1])\n",
    "total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_as_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-3. prediction & accuracy 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* logits 을 softmax 처리후 argmax 함수를 이용하여, [10240, 1] 로 만든다.\n",
    "* [10240, 1] 크기의 prediction 과 output 을 [512, 20] 로 만든다.\n",
    "* 비교함수를 이용하여 정확도를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "predictions = tf.reshape(predictions, [-1, step_size])\n",
    "\n",
    "target = tf.reshape(y_as_list, [-1, step_size])\n",
    "\n",
    "matching = tf.reduce_all(tf.equal(predictions, tf.cast(target, tf.int64)), axis=1)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(matching, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4. training algorithm 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(0.1)\n",
    "train_step = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 100 번의 epoch 를 사용하여 훈련, 정확도가 100% 가 되면 훈련 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  0 , loss : 6.059e-01 , acc : 0.002\n",
      "Epoch -  1 , loss : 5.442e-01 , acc : 0.007\n",
      "Epoch -  2 , loss : 4.710e-01 , acc : 0.106\n",
      "Epoch -  3 , loss : 4.300e-01 , acc : 0.428\n",
      "Epoch -  4 , loss : 3.513e-01 , acc : 1.449\n",
      "Epoch -  5 , loss : 3.049e-01 , acc : 3.798\n",
      "Epoch -  6 , loss : 2.742e-01 , acc : 7.110\n",
      "Epoch -  7 , loss : 2.297e-01 , acc : 12.664\n",
      "Epoch -  8 , loss : 2.324e-01 , acc : 22.085\n",
      "Epoch -  9 , loss : 1.609e-01 , acc : 36.193\n",
      "Epoch -  10 , loss : 1.165e-01 , acc : 50.917\n",
      "Epoch -  11 , loss : 8.835e-02 , acc : 62.770\n",
      "Epoch -  12 , loss : 7.257e-02 , acc : 71.006\n",
      "Epoch -  13 , loss : 1.663e-01 , acc : 79.392\n",
      "Epoch -  14 , loss : 4.213e-02 , acc : 87.564\n",
      "Epoch -  15 , loss : 3.057e-02 , acc : 94.948\n",
      "Epoch -  16 , loss : 2.582e-02 , acc : 94.822\n",
      "Epoch -  17 , loss : 2.207e-02 , acc : 97.892\n",
      "Epoch -  18 , loss : 1.966e-02 , acc : 97.478\n",
      "Epoch -  19 , loss : 1.774e-02 , acc : 97.663\n",
      "Epoch -  20 , loss : 1.608e-02 , acc : 98.257\n",
      "Epoch -  21 , loss : 1.487e-02 , acc : 97.919\n",
      "Epoch -  22 , loss : 1.427e-02 , acc : 98.328\n",
      "Epoch -  23 , loss : 1.247e-02 , acc : 99.737\n",
      "Epoch -  24 , loss : 1.210e-02 , acc : 97.998\n",
      "Epoch -  25 , loss : 1.090e-02 , acc : 99.807\n",
      "Epoch -  26 , loss : 1.057e-02 , acc : 97.935\n",
      "Epoch -  27 , loss : 9.715e-03 , acc : 99.851\n",
      "Epoch -  28 , loss : 9.075e-03 , acc : 99.868\n",
      "Epoch -  29 , loss : 9.195e-03 , acc : 97.879\n",
      "Epoch -  30 , loss : 8.356e-03 , acc : 99.881\n",
      "Epoch -  31 , loss : 7.860e-03 , acc : 99.903\n",
      "Epoch -  32 , loss : 7.439e-03 , acc : 99.909\n",
      "Epoch -  33 , loss : 7.609e-03 , acc : 97.856\n",
      "Epoch -  34 , loss : 7.014e-03 , acc : 99.914\n",
      "Epoch -  35 , loss : 6.641e-03 , acc : 99.921\n",
      "Epoch -  36 , loss : 6.334e-03 , acc : 99.933\n",
      "Epoch -  37 , loss : 6.073e-03 , acc : 99.942\n",
      "Epoch -  38 , loss : 6.522e-03 , acc : 97.510\n",
      "Epoch -  39 , loss : 5.925e-03 , acc : 99.931\n",
      "Epoch -  40 , loss : 5.625e-03 , acc : 99.946\n",
      "Epoch -  41 , loss : 5.393e-03 , acc : 99.952\n",
      "Epoch -  42 , loss : 5.193e-03 , acc : 99.955\n",
      "Epoch -  43 , loss : 5.012e-03 , acc : 99.960\n",
      "Epoch -  44 , loss : 4.847e-03 , acc : 99.961\n",
      "Epoch -  45 , loss : 5.962e-03 , acc : 95.873\n",
      "Epoch -  46 , loss : 4.996e-03 , acc : 99.936\n",
      "Epoch -  47 , loss : 4.695e-03 , acc : 99.954\n",
      "Epoch -  48 , loss : 4.502e-03 , acc : 99.963\n",
      "Epoch -  49 , loss : 4.348e-03 , acc : 99.966\n",
      "Epoch -  50 , loss : 4.213e-03 , acc : 99.967\n",
      "Epoch -  51 , loss : 4.092e-03 , acc : 99.969\n",
      "Epoch -  52 , loss : 3.980e-03 , acc : 99.972\n",
      "Epoch -  53 , loss : 3.876e-03 , acc : 99.974\n",
      "Epoch -  54 , loss : 3.779e-03 , acc : 99.976\n",
      "Epoch -  55 , loss : 3.688e-03 , acc : 99.976\n",
      "Epoch -  56 , loss : 3.602e-03 , acc : 99.977\n",
      "Epoch -  57 , loss : 3.520e-03 , acc : 99.977\n",
      "Epoch -  58 , loss : 3.442e-03 , acc : 99.977\n",
      "Epoch -  59 , loss : 3.368e-03 , acc : 99.979\n",
      "Epoch -  60 , loss : 3.298e-03 , acc : 99.978\n",
      "Epoch -  61 , loss : 3.231e-03 , acc : 99.979\n",
      "Epoch -  62 , loss : 3.167e-03 , acc : 99.979\n",
      "Epoch -  63 , loss : 3.106e-03 , acc : 99.980\n",
      "Epoch -  64 , loss : 3.048e-03 , acc : 99.980\n",
      "Epoch -  65 , loss : 2.992e-03 , acc : 99.981\n",
      "Epoch -  66 , loss : 2.938e-03 , acc : 99.981\n",
      "Epoch -  67 , loss : 2.886e-03 , acc : 99.981\n",
      "Epoch -  68 , loss : 2.836e-03 , acc : 99.982\n",
      "Epoch -  69 , loss : 2.788e-03 , acc : 99.982\n",
      "Epoch -  70 , loss : 2.741e-03 , acc : 99.982\n",
      "Epoch -  71 , loss : 2.696e-03 , acc : 99.983\n",
      "Epoch -  72 , loss : 2.653e-03 , acc : 99.983\n",
      "Epoch -  73 , loss : 2.610e-03 , acc : 99.983\n",
      "Epoch -  74 , loss : 2.570e-03 , acc : 99.984\n",
      "Epoch -  75 , loss : 2.530e-03 , acc : 99.984\n",
      "Epoch -  76 , loss : 2.492e-03 , acc : 99.985\n",
      "Epoch -  77 , loss : 2.455e-03 , acc : 99.985\n",
      "Epoch -  78 , loss : 2.419e-03 , acc : 99.986\n",
      "Epoch -  79 , loss : 1.569e-02 , acc : 94.531\n",
      "Epoch -  80 , loss : 3.318e-03 , acc : 99.772\n",
      "Epoch -  81 , loss : 2.749e-03 , acc : 99.976\n",
      "Epoch -  82 , loss : 2.552e-03 , acc : 99.984\n",
      "Epoch -  83 , loss : 2.447e-03 , acc : 99.986\n",
      "Epoch -  84 , loss : 2.375e-03 , acc : 99.987\n",
      "Epoch -  85 , loss : 2.319e-03 , acc : 99.987\n",
      "Epoch -  86 , loss : 2.271e-03 , acc : 99.988\n",
      "Epoch -  87 , loss : 2.229e-03 , acc : 99.988\n",
      "Epoch -  88 , loss : 2.191e-03 , acc : 99.989\n",
      "Epoch -  89 , loss : 2.155e-03 , acc : 99.989\n",
      "Epoch -  90 , loss : 2.122e-03 , acc : 99.990\n",
      "Epoch -  91 , loss : 2.090e-03 , acc : 99.990\n",
      "Epoch -  92 , loss : 2.061e-03 , acc : 99.990\n",
      "Epoch -  93 , loss : 2.032e-03 , acc : 99.990\n",
      "Epoch -  94 , loss : 2.005e-03 , acc : 99.990\n",
      "Epoch -  95 , loss : 1.979e-03 , acc : 99.990\n",
      "Epoch -  96 , loss : 1.953e-03 , acc : 99.990\n",
      "Epoch -  97 , loss : 1.929e-03 , acc : 99.991\n",
      "Epoch -  98 , loss : 1.905e-03 , acc : 99.991\n",
      "Epoch -  99 , loss : 1.882e-03 , acc : 99.991\n"
     ]
    }
   ],
   "source": [
    "no_of_batches = int(len(train_input)/batch_size)\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    ptr = 0\n",
    "    avg_acc = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp, out = train_input[ptr:ptr+batch_size], train_output[ptr:ptr+batch_size]\n",
    "        ptr+=batch_size\n",
    "        _, lss, acc = sess.run([train_step, total_loss, accuracy], feed_dict={x: inp, y: out})\n",
    "        avg_acc += acc / no_of_batches\n",
    "        \n",
    "    print(\"Epoch - \",str(i), ', loss : %.3e' % lss, ', acc : %.3f' % (avg_acc * 100))\n",
    "       \n",
    "    if avg_acc == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-5. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size: 786431 , correct count: 786372\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "incorrect_idx_array = []\n",
    "for idx in range(int(np.ceil(len(test_input) / batch_size))):\n",
    "    s = idx * batch_size\n",
    "    e = s + batch_size\n",
    "    \n",
    "    match = sess.run(matching, feed_dict={x: test_input[s:e], y: test_output[s:e]})\n",
    "    \n",
    "    count += np.sum(match)\n",
    "    incorrect_idx_array = np.concatenate([incorrect_idx_array, np.where(match == False)[0] + s]).astype(int)\n",
    "    \n",
    "print('test set size: %d' % (len(test_input)), ', correct count: %d' % count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실패한 data set 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "input     : 01010010111111111111\n",
      "output    : 01010011000000000000\n",
      "prediction: 11010011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00011101001111111111\n",
      "output    : 00011101010000000000\n",
      "prediction: 00011101011000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01110011111111111111\n",
      "output    : 01110100000000000000\n",
      "prediction: 01110110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01111111111111111111\n",
      "output    : 10000000000000000000\n",
      "prediction: 11111000000001010000\n",
      "correct: False\n",
      "\n",
      "input     : 11111000000111111111\n",
      "output    : 11111000001000000000\n",
      "prediction: 11111000001000010000\n",
      "correct: False\n",
      "\n",
      "input     : 01110111111111111111\n",
      "output    : 01111000000000000000\n",
      "prediction: 00111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11110111111111111111\n",
      "output    : 11111000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10000011111111111111\n",
      "output    : 10000100000000000000\n",
      "prediction: 10000110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10101111111111111111\n",
      "output    : 10110000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11000111111111111111\n",
      "output    : 11001000000000000000\n",
      "prediction: 11011100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11100111111111111111\n",
      "output    : 11101000000000000000\n",
      "prediction: 11111100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00111011111111111111\n",
      "output    : 00111100000000000000\n",
      "prediction: 10111100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01000111111111111111\n",
      "output    : 01001000000000000000\n",
      "prediction: 01011100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00111001111111111111\n",
      "output    : 00111010000000000000\n",
      "prediction: 00111011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00100011111111111111\n",
      "output    : 00100100000000000000\n",
      "prediction: 00100110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11010011111111111111\n",
      "output    : 11010100000000000000\n",
      "prediction: 11010110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00011001111111111111\n",
      "output    : 00011010000000000000\n",
      "prediction: 00011011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11011101001111111111\n",
      "output    : 11011101010000000000\n",
      "prediction: 11011101011000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10100111111111111111\n",
      "output    : 10101000000000000000\n",
      "prediction: 10111100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00110011111111111111\n",
      "output    : 00110100000000000000\n",
      "prediction: 00110110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00000001111111111111\n",
      "output    : 00000010000000000000\n",
      "prediction: 00000011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10001001111111111111\n",
      "output    : 10001010000000000000\n",
      "prediction: 10001011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10001111110111111111\n",
      "output    : 10001111111000000000\n",
      "prediction: 00001111111000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00000011111111111111\n",
      "output    : 00000100000000000000\n",
      "prediction: 00000110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01100011111111111111\n",
      "output    : 01100100000000000000\n",
      "prediction: 01100110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01000011111111111111\n",
      "output    : 01000100000000000000\n",
      "prediction: 01000110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00011011111111111111\n",
      "output    : 00011100000000000000\n",
      "prediction: 10011100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00001001111111111111\n",
      "output    : 00001010000000000000\n",
      "prediction: 00001011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00101011011111111111\n",
      "output    : 00101011100000000000\n",
      "prediction: 00101011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01101111111111111111\n",
      "output    : 01110000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01001111111111111111\n",
      "output    : 01010000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10001011011111111111\n",
      "output    : 10001011100000000000\n",
      "prediction: 10001011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10001111111111111111\n",
      "output    : 10010000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11011111111111111111\n",
      "output    : 11100000000000000000\n",
      "prediction: 11111000000001000000\n",
      "correct: False\n",
      "\n",
      "input     : 10101111110111111111\n",
      "output    : 10101111111000000000\n",
      "prediction: 00101111111000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11001011011111111111\n",
      "output    : 11001011100000000000\n",
      "prediction: 11001011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10100001111111111111\n",
      "output    : 10100010000000000000\n",
      "prediction: 10100011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00111111111111111111\n",
      "output    : 01000000000000000000\n",
      "prediction: 10111000000001000000\n",
      "correct: False\n",
      "\n",
      "input     : 11001001111111111111\n",
      "output    : 11001010000000000000\n",
      "prediction: 11001011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00100111111111111111\n",
      "output    : 00101000000000000000\n",
      "prediction: 00111100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10110011111111111111\n",
      "output    : 10110100000000000000\n",
      "prediction: 10110110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01011111111111111111\n",
      "output    : 01100000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00000111111111111111\n",
      "output    : 00001000000000000000\n",
      "prediction: 10011100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00001011111111111111\n",
      "output    : 00001100000000000000\n",
      "prediction: 10001100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00001111111111111111\n",
      "output    : 00010000000000000000\n",
      "prediction: 10011000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01011001100011111111\n",
      "output    : 01011001100100000000\n",
      "prediction: 01011001100000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00010011111111111111\n",
      "output    : 00010100000000000000\n",
      "prediction: 00010110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00101111111111111111\n",
      "output    : 00110000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01000001111111111111\n",
      "output    : 01000010000000000000\n",
      "prediction: 01000011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10100011111111111111\n",
      "output    : 10100100000000000000\n",
      "prediction: 10100110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01101001111111111111\n",
      "output    : 01101010000000000000\n",
      "prediction: 01101011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10011111111111111111\n",
      "output    : 10100000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10101011011111111111\n",
      "output    : 10101011100000000000\n",
      "prediction: 10101011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01100111111111111111\n",
      "output    : 01101000000000000000\n",
      "prediction: 01111100000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 00011111111111111111\n",
      "output    : 00100000000000000000\n",
      "prediction: 10111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 01010011111111111111\n",
      "output    : 01010100000000000000\n",
      "prediction: 01010110000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11101111111111111111\n",
      "output    : 11110000000000000000\n",
      "prediction: 11111000000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 11101011011111111111\n",
      "output    : 11101011100000000000\n",
      "prediction: 11101011000000000000\n",
      "correct: False\n",
      "\n",
      "input     : 10000001111111111111\n",
      "output    : 10000010000000000000\n",
      "prediction: 10000011000000000000\n",
      "correct: False\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "if len(incorrect_idx_array) != 0:\n",
    "    inp, out = test_input[incorrect_idx_array], test_output[incorrect_idx_array]\n",
    "else:\n",
    "    inp, out = [[1] + [0] * 19, [0] + [1] * 19], [[0, 1] + [0] * 18, [1] * 20]\n",
    "    \n",
    "    \n",
    "pred, match = sess.run([predictions, matching], feed_dict={x: inp, y: out})\n",
    "\n",
    "print('='*30)\n",
    "for a, b, c, d in zip(inp, out, pred, match):\n",
    "    print('input     : ', end='')\n",
    "    for o in a[::-1]:\n",
    "        print(o, end='')\n",
    "    print()\n",
    "    print('output    : ', end='')\n",
    "    for o in b[::-1]:\n",
    "        print(o, end='')\n",
    "    print()\n",
    "    print('prediction: ', end='')\n",
    "    for o in c[::-1]:\n",
    "        print(o, end='')\n",
    "    print()\n",
    "    print('correct: %r' % d)\n",
    "    print()\n",
    "print('='*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
